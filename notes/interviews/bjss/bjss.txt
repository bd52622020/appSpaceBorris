
----------------------------------------------------------------------------------------------------
                                          BJSS
----------------------------------------------------------------------------------------------------
Who is bjss?
------------
Leading technology and engineering consultancy for innnovative technology and engineering solutions

  -Digital transformation and user engagement a reality.
  -Strategy, software, cloud, data and automation.

DataOps
-------
Analysts, data scientists, developers and operations working together in the entire service life 
cycle, from design to development to production support. DataOps make use of similar techniques 
compared to DevOps and its built on 5 primary pillars.

  -Focus on creating data products
  -Operationalizing analytics and data science 
  -Data science and analytics recipes and blueprints 
  -Methodologies and processes 
  -Culture allignment and clarity

Project description
-------------------
As a BJSS data engineer you'll help our clients deploy data pipelines and processes in a production
safe manner, using the latest technologies and with a DataOps culture.

  -Work in a fast moving, agile environment, within muilti-disciplinary teams, deliverying modern
   data platforms into large organisations.
  -Expertise and confidence to work collaboratively with engineers, architects and business analysts
   in multi-disciplinary teams on client site.

Experience 
----------
-Python
-AWS or Azure data services (E.g. Data factory, Synapse, Redshift, etc)
-Databricks or Apache Spark
-Distributed NoSQL database --> HBase
-Stream processing technologies such as Kafka
-Hadoop ecosystem exposure

AWS
---
Global cloud platform which allows you to host and manage services in the internet. 

  -Infrastructure as a service (servers)
  -Platform as a service
  -Software as a service
  -Cloud storage platform

Services overview
-----------------
-EC2: Elastic compute cloud, bare servers which you can launch and run software
-VPC: Virtual private cloud, lets you create networks in the cloud and run your servers on those 
      networks.
-S3: Simple storage service, opportunity to upload and share files.
-RDS: Relational Database service, SQL, MySql, Postgres, Route 53.
-ELB: Elastic load balancer, service which balances incoming traffic to multiple machines.
  -Autoscaling: Adds capacity in real time when load is increased.

EC2
---
One of the most popular of AWS offerings, the first step is to create an Amazon machine image(AMI),
select the type (CPU, Memory, storage, network performance) and add storage (EBS) volume.
The security group, set firewall rules controlling the traffic for the instance, also create a 
key/pair to access the new instance.
  -Renting virtual machines 
  -Storing data on virtual drives (EBS)
  -Distributing load across machines (ELB)
  -Scaling the services using an auto-scaling group (ASG)

EC2 User Data
-------------
EC2 user data is used to automate boot tasks such as:
  -Installing updates
  -Installing software
  -Downloading common files from the internet
-It is possible to bootstrap our instances using an EC2 User data script
-Bootstrapping means launching commands when a machine starts 
-That script is only run once at the instance first start

IAM
---
Identity and access management, the whole AWSD security is there with the following categories. Root
account should never be used, users must be created with proper permissions. Policies are written in
JSON (JavaScript Object Notation)
  -Users: Physical person
  -Groups: By functions such as admins, devops or teams such as engineering, design ,etc
  -Roles: Internal usage within AWS resources, for machines

*IAM has a global view, permissions are governed by policies, you can set up MFA (multi factor 
authentication). It's best to give users the minimal amount of permissions they need.
*IAM federation:  Big enterprises usually integrate their own repository of users with IAM, allowing
 one to login into AWS using their company credentials. Identity federation uses the SAML standard
 (Active directory) 
  -One IAM user per physical person
  -One IAM role per application

Redshift
--------
Data warehouse, easy go gain insights from all your data. With redshift you can query petabytes of structured
and semi-structured data across your data warehouse, operational database, and your data lake using standard SQL.
  
  -Redshift lets you easily save the results of your queries back to your S3 data lake using open formats like
   apache Parquet to further analyse from other analytics services.
  -Ability to query data directly into our Amazon S3 data lake, we have been able to easily integrate new data 
   sources in hours, not days or weeks. Resulting in reduced time for insight and help control infrastructure cost.
  -Performance, Amazon Redshift is the fastest cloud data warehouse.
  -Bring together structured data from your data warehouse and semi-structured data such as application logs from
   your S3 data lake to get real-time operational insight on your applications and systems.

Architecture
------------
Client application: Interacts using drivers.
  -JDBC: Java database connectivity
  -ODBC: Open database connectivity

Leader node: Manages interaction between the client application and compute nodes, it analyses and developes
             designs in order to carry out database operations.

Compute node: Executes the program and shares result back to the leader node for final aggregation.
  -Node slices: Processsing, each slice is alloted with specific memory space, where it processes its workload.
                They work in parallel in order to finish their work.

-Column Storage: Column storage is an essential factor in optimizing query performance and resulting in quicker
outputs.

-Compression: Compression is a coumn level operation which decreases storage requirements and improves query 
performance.


Demo
----
1) Set up prerequesites:
  -Sign up/login to AWS
  -Determine firewall rules
2) Create an IAM role for amazon redshift
  -Sign in to AWS management console 
  -Create a role
  -Chose redshift 
  -Add permission policies 
  -Add tags if necessary
  -Save the role ARN for the role 
3) Launch redshift cluster
4) Authorize access to the cluster
  -Configure the VPC security group
5) Connect to the cluster and run queries 
  -AWS management console with query editor 
  -Optionally download and set a SQL client application
6) Load data from Amazon S3
  -Create tables
  -Load using copy command 

---------------
Potential cases
---------------
OLAP applications look at multiple records at the same time. You save memory because you fetch just the columns
of data you need instead of whole rows and since data is stored via column, all data is of the same data-type 
allowing for easy compression.

-------------------
Potential questions
-------------------

1) Tell me about youself
------------------------
I am an experienced Big data engineer primarily focused on data ingestion using distributed systems
in the Hadoop ecosystem. In my position working at Data reply as a big data engineer, I worked on the
inplementation of RDBMS data ingestion, batch processes and queries into messaging systems such as kafka.
The implementation I have worked on falls into monitoring and testing data using scripts, development of 
spark consumers to clean, test and perform aggregations using scala and python scripts. Also extensive use
of kafka for the development of pipelines, configuration of producers from API calls, system logs and data 
migration from RDBMS systems.

2) Rescent project
-----------------
One project that I was heavily involed was in the a pipeline to centalize system logs. The company provide
information solutions, primarily testing performance, quality of data and potential opportunities for change.
Having multiple systems they wanted a centralized storage unit. Using HDFS as the foundation of the pipeline,
I took part in ingestion scripts from RDBMS systems and logs from their systems. Cleaning data following 
specifications and using spark and involvement in testing the pipeline.

3) Previous experience
----------------------
During Appnovation, I took assisstance in various aspects of data, for example the projects included data 
migration, modeling and warehousing. Providing data solutions when traditional RDBMS systems lack capacity.
  -Clickstream data: analyse UI performance, data where the user is clicking, how the user interacts 
  -Live stream data: Video sources stored
  -User information: build a profile

4) Cloud AWS
------------
EC2
IAM
RedShift


-Contract only
-Intership

-volume - type

1 level - explaining what I did
2 level - detailed information about the project
3 level - code level, specific functions


5) Amazon Redshift
------------------
Migrations to the cloud, existing premise warehouses dont provide the flexibility and scale to meet the exponential
growth of data also cost of renewing hardware.

  -Rise of event data, unstructured data.
  -IOT, streaming data, event data, log data.

Challenges of data analytics at scale:
  -Data volume, variety, velocity
  -Performance, concurrency
  -Multiple analytics needs
  -Security governance 
  -Increasingly costly, inflexible

Real-time lake house approach
-----------------------------
Combines traditional data sources in data warehouse with event data sitting in object store such as Amazon S3
as well as real-time data stored in a transactional database such as Amazon Aurora or Amazon RDS.

  -Single interface through a common redshift query/dashboard, with a set of tables where the data resides, such
   S3 parquet format, CVS format. Or transactional data loaded in real-time into a postGres database such as 
   Amazon RDS or Aurora.
  -Unique value of redshift is that we dont have to load all the data into redshift database in order to query it.
  -Query federation model, where data can be local, S3, postgress
  -The analysts only communicate with tables and schemas, from an data engineering perspective there is no need 
   to move the data but simply query the data in place.
  -Single view providing the flexibility and scalability to have insights into all the organization's data.

Benefits 
--------
-Integration with Data lake: And other AWS services, such as Lake formation catalog & security, Cloud watch for 
 monitoring, data warehouse migration service and other integration.
-Performance: Faster than other cloud data warehouses
-Lower cost: Up to 75% less 
-Most scalable: Virtually unlimited elastic linear scaling, concurrency scaling 
-Most secure & compliant: VPC, encryption, cloud trail and certifications such as SOC, PCI, ISO and FedRAMP
-Easy to manage: Easy to provision & manage, automated backups.

Features
--------
-Concurrency scaling
-Spatial processing
-Stored procedures
-Elastic resize
-IAM role chaining
-Auto analyze

Architecture
------------
SQL clients/BI Tools, you are connecting to the leader node using JDBC/ODBC, when you submit a query on redshift,
each compute node is working in parallel to execute the query. Copy/load data from S3 in parallel.

Leader node:
  -SQL endpoint
  -Stores metadata
  -Coordinates parallel SQL processing & ML optimizations
Compute node:
  -Local, columnar storage
  -Executes queries in parallel
  -Load, unload, backup, restore from S3
Redshift spectrum:
  -Elastic fleet of compute nodes
  -Ability to query data in any open format, JSON, CSV or Avro
  -Ability to transport data from S3 to redshift cluster
Amazon S3:
  -Storage of data

Turbo
-----
Charges query performance with machine learning based automatic optimizations.

  -Automates table maintenance 
  -Optimizes for peak performance as data and workloads scale
  -Offers perscriptive recommendations with ability to apply changes dynamically

Amazon Redshift advisor
-----------------------
  -Offers specific customized recommendations to improve performance and decrease the operating costs for your
  cluster

Data models
-----------
-Star
-Snowflake
-Highly denormalized 

Data loading:
Copy command: Loads data into Redshift in parallel, specifying the table name, path, aws credentials and region.
  -Copy <tablle> from <location> credentials "aws_acesskey_id=<access_key;secretkey=..." iam_role "arn" region;
  -Copy customer from "s3://mydata" credentials "awsacces...", delimeter "|";
Materialized Views: Compute once query many times 
  -Speed up queries by orders of magnitude
    -Joins, filters, aggregations, and projections
  -Simplify and accelerate ETL/BI pipelines
    -Incremental refresh
    -User triggered maintenance 

NoSQL
-----
*High availability
  -load balancer, retrieve information from other machines 
  -Distributed system
  -Everything is stored in json packets, key-value pairs, extremly fast read 
  
Streaming
  -Multinode cluster
  -Kafka - multinode

Python
  -How many I used
  -Frameoworks
  -
