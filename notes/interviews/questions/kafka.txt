
------------------------------------------------------------------------------------------------
                                              Kafka 
------------------------------------------------------------------------------------------------

1)What is kafka?
----------------
Kafka is a distributed streaming platform, leaverages the advantages of storing and processing 
data while its in different locations and as a streaming service it processes data between source
and consumer.

2)Features of Kafka
-------------------
Kafka improves fault tolerance when processing data in real-time due to its replication system,
It simplifies horizontal scaling, as a distributed system in order to increase performance you
can add more kafka brokers. And Kafka permits free data handling and distribution by decoupling
the data inside the messages, Kafka does not care what kind of data is being pased but can be 
validated by the use of schemas such as AVRO.

3)Architecture
--------------
The producer can be any source of data that is pushed to a Kafka cluster, Kafka cluster contains 
brokers that store the data inside of topics, topics are broken down into partitions and are 
replicated three times by default and consumers subscribe to a topic to request data. There can
be multiple consumers working together, they are called consumer groups and they allow for much 
higher processing power.

4)Partitions
------------
Topics are divided into partitions, each partition contains records which are in an unchangable
sequence, each record has a unique offset after its assigned into a partition.

Topics are broken down into partitions by design, when data (messages) are sent into a partition,
the timestamp is the default sorting criterion from old to new. The new data has an offset value 
to define their position, starting from zero the values increase by a factor of 1. Kafka runs 
with the help of Zookeeper, managing configurations, naming policies and grouping 

5)Consumer
----------
Each consumer/group accesses data from the partition on demand, the consumer pulls data through 
a query. The consumers range from databases to dashboards using the data for real-time management

6)Kafka APIs
------------
There are five core APIs:
Producer API: Controls who can gain write permission to topics and partitions in a Kafka cluster
Connector API: Allows to create and maintain live transactions between both the consumer/producer
Admin API: For adminitrative purposes, allows to inspect and change components in the cluster
Streams API: Processing calls manager to arrange queries data from publishers into usable outputs
  topics for the consumer 
Consumer API: When a request is sent, access to topics are granted

7)Replication factor
--------------------
The replication protocol with the distributed nature of Kafka, allows for a certain degree of 
resilience. This can be difficult to achieve due to replicas jumping in and out of the in sync
replica list (ISR).

Every partition has a write-ahead log where the messages are stored with a unique offset to 
identify its position, allowing Kafka to automatically recover in the presence of failures. Out
of all the replicas, one replica is designated as the leader while the others are followers, the
leader takes writes from the producer and the followers copy the leader.

The replication factor guarantees that if a leader fails, the followers should be "in sync" with
the leader. The leader for every partition tracks this in-sync replica list by computing the log
of every replica from itself, a message is commited only after it has been successfully copied 
to all the in-sync replicas. 

8)Topics 
--------
Topics can be defined as categories which records are stored and published, records published to
the cluster have a retention period. Producers take data from a source and stores it to a topic 
in the form of a message, a message can include any type of information. Records are byte arrays
that stores any format and has four attributes, key and value are mandatory but other attributes
such as timestamp and headers are optional.

Kafka retains records in the log, the consumers are responsible for tracking the position in the
log, known as the "offset". Typically the consumer advances the offset in a linear manner but in
case of reprocessing, the consumer can reset to an older offset.

9)Broker
--------
A Kafka cluster is composed of one or more servers, a server being a broker. Producers push
records into Kafka topics within the broker and a consumer pulls records off a Kafka topic. The 
management of the brokers in a cluster is performed by Zookeeper and there can be multiple 
instances of Zookeeper running with a recommened of 3 to 5.

10)High throughput
------------------
Kafka achives high throughput by its architecture, the Kafka cluster, producer and consumer. In 
In the Kafka cluster there are many brokers, these brokers have many partitions which is the unit 
that is distributed to brokers and is also used to achieve parallelism. To increase higher throughput
you need to increase more producer/broker/consumers.

Optimized the access pass of a single partition, the data is stored in a high level data structure
or commit log. The commit log is an append only system with each message being placed sequentially 
with a unique offset ID. A consumer can read from multiple points in the offsets and continue reading
data sequentially, these sequential approach allows the system to achieve effective caching to 
optimize performance.

Bashing: Typically during sending messages over Kafka, instead of trying to send a 
single message RPC request, it tries to buffer as many mesages as possible in the cache and either
enough messages has been accumulated or a specific time period has passed. Sending/Receiving those 
batch messages, there is the need to flush those messages to disk for persistence but it is typically
done periodically using batch.

Compression: Typically specified in the producer side, compressing those batch messages into a unit 
providing better performance, stored in a compressed format into disk and when it needs to be read 
the message is decompressed.
 
11)Reliability and durability
-----------------------------
Redundancy is achieved by replication, when a topic is created there is a certain amount of replicas
Unlike quorum based replication that can only tolerate a minority number of failures, Kafka saves
the data in an actual copy to tolerate failure. 

Replication: When a topic is created with replicas, those replicas need to be assigned to brokers,
spreading the replicas evenly across all the brokers to avoid data loss in case of broker failure.
In Kafka the replicas are strongly consistent, meaning that they are identical to the original at any
given point in time. To achieve this, one of the replicas will become the leader and the rest will 
be follower. The producer to publish a message will communicate with the leader and the follower will
only copy the follower. The leader waits until the followers have commited the messages before it
declares that it has been successfully commited.

  -The producer can choose the receive acknowledgement when a message has been produced, with no
  acknoledgement, only the leader and waiting for all partitions to commit the message. There is a 
  trade-off between efficiency and reliability.
  -The consumer will only be able to read the messages that have been successfully commited.
  -The leader has more load compared to followers, to achieve balance, the leaders will be distributed
  across all the brokers as evenly as possible.

Replication Protocol: ISR (In-Sync replicas), replicas reads from the leader's log file. A replica
is considered in-sync with the leader as long as it keeps reading from the end of the leader's log 
within a time-frame.

Follower failure: Reduce replicas from set, having fewer replicas but the producer will continue to
produce messages because it does no longer creates a error message of under-replicaed messages. When 
the follower comes back, it finds the leader and starts to catch up the messages and only until the
replica is caught back then it will be join the in-sync replica set.

Leader failure: Figure out a new leader, the broker get elected by selecting an in-sync replica set.
Only counting replicas that are up to date so no data loss will happen.

Unclean leader election: When a new leader gets elected but does not have all the messages up to date
but due to lack of other options, brokers with incomplete data will be elected as leaders and there
will be some data loss. In order to avoid data loss, you can dissable the settings and the system 
will wait for an in-sync replica set to comeback but you lose high availability.

Guarantee replicas: mean-in-sync will not allow messages to be commited untill the number of replicas
is meet.

Leader election: Console chord controller, in a Kafka cluster, one of the brokers is a court 
controller. It monitors the liveness of each broker, acting on moving the leaders to other brokers.
Communicating the new leader and new in-sync replicas with other Brokers. IF the Broker will the 
controller fails, a new Broker will elected to serve that function.

12)Compacted topic
------------------
In situations when messages have the same key, after a round with compaction, the topic will only be
retaining those messages whose key is the last message.   
 Commit log: Each messages has an offset, key and value
  -Offset
  -Key
  -Value


13)Commit log
-------------
Each commit log is divided into segments, each segment stores a non-overlapping but consecutive range
of offsets, each segment is composed of three parts. The first part is the block file, the offset 
index and the time step index.

14)Protocols
-Pull protocol
-Protocol 
