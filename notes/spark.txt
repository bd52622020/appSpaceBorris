				Spark 

Spark is the cluster computing framework for large-scale data processing.
Used for processing and analysing data in a distributed manner and process
the data in parallel.

*OLAP: Online Analytical processing.

Unified: There is no need to piece together an application out of multiple
	APIs or systems. Spark provides built-in APIs.

Computing Engine: Spark handles the loading of data from various file systems
	and runs computations on it, but does not store any data permanently.
	Spark operates entirely in memory.
	-Performance
	-Speed

The spark Application
---------------------

Every Spark application consists of a Driver and set of destributed 
worker processes (Executors)

1) Spark Driver: The driver runs the main() method of our application
   and is where the SparkContext is created.

	-Runs on a node in the cluster or on a client, and schedules
	 the job execution with a cluster manager.
	
	-Responds to user's program or input.
	
	-Analyzes, schedules, and distributes work across the executors.
	
	-Stores metadata about the running application and conveniently
	 exposes it in a webUI.

2) Spark Executors: An executor is a distributed process, reponsible for 
   the execution of tasks. Each spark application has its own set of 
   executors, which stay alive for the life cycle of a single application.

	-Perform all the data processing of a Spark job.

	-Stores results in memory, only persisting to disk when 
	 specifically instructed by the driver program.

	-Returns results to the driver once they have been completed.

	-Each node can have anywhere from one executor per node to 
	 one executor per core.


Spark Application Workflow
--------------------------

1) The standalone application is kicked off, and initializes its SparkContext.
   Only after having a SparkContext can an app be referred to as a driver.

2) Our driver program asks the cluster manager for resources to launch its 
   executors.

3) The cluster manager launches the executors.

4) The driver runs the actual Spark code.

5) Executors run tasks and send their results back to the driver.

6) SparkContext is stopped and all executors are shut down, returning 
   resources back to the cluster.

Spark Architecture Overview
---------------------------

Spark has a well-defined layered architecture, with loosely coupled
components, based on two primary abstractions:

-RDDs: Resilient Distributed Datasets
-DAG: Directed Acyclic Graph

* Resilient Distributed Datasets(RDD)

RRDs are essentially the building blocks of Spark, even Sparks higher-level
APIs (DataFrames, Datasets) are composed of RDDs under the hood.

	-Resilient: Running a cluster of machines, data-loss from hardware
	 failure is a real concern, RDDs are fault tolerant and can rebuild
	 themselves in the event of failure.

	-Distriburted: A single RDD is stored on a series of different nodes
	 in the cluster, belonging to no single source and no simple point
	 of failure. Enabling the cluster to operate on the RDD in parallel.

	-Dataset: A collection of values.


How does it work:

All data in Spark will be stored inside some form of RDD
Spark offers a slew of "High Level" APIs built on top of RDDs
designed to abstract away complexity, namely the DataFrame and Dataset.

A JavaRDD<string> is essentially just a list<string> dispersed amongst each
node in the cluster, with each node getting several different chunks of the
list.

RDDs work by splitting up their data into a series of partitions to be stored
on each executor node. Each node will then perform its owrk only on its own
partitions. If an executor fails, Spark can rebuild the partition needed from
the original source and re-submit the task for completion.


Operations:

RDDs are immutable, once created, they cannot be altered in any way. They can
only be transformed. Spark jobs can be composed of:
	-Loading data into an RDD.
	-Tranforming an RDD.
	-Performing an action on an RDD.

Spark defines a set of APIs for working with RDDs that can be broken down into
two large groups.
	-Transformations: Create new RDD from existing one.
	-Actions: Return a value to the driver program after computation
	 on its RDD through a function.


Lazy Evaluation:

All transformations in Spark are lazy. When an RDD is created in Spark by 
transformation of an existing RDD, it won't generate that dataset until a
specific action is performaed on it or one of its children.

Spark will then perform the transformation and the action that triggered it.


* Directed Acyclic Graph(DAG)

Whenever an action is performed on an RDD, Spark creates a DAG, a finite
direct graph with no directed cycles.

Graph: A series of connected vertices and edges

Each vertex in the DAG is a Spark function, some operation performed on a RDD.
E.g. map, mapToPair, reduceByKey, etc.

In MapReduce, the DAG consists of two vertices: Map --> Reduce.


Latency
-------
*Operations made in memory
*Operations that require sending data over the network


Shuffling
---------
Distributed data: when applying a groupBy or a groupByKey
-ShuffleRDD

groupByKey: Collects all of the values associated with the given key and stores
	    them in that single collection. Meaning the data has moved around the
	    network.

* Moving the data on the network is called shuffling.
	-Enormous hit to performance
	-Moving the data around the network-latency

To do a distributed groupByKey, you move data between nodes so that the data can
be collected together with its key in a regular, normal single machine Scala 
collection 

* Reduce on the mapper before doing the shuffle: Reduce the amount of data that it
needs to send over the network. 
 
Benefits: By reducing the dataset first, the amount of data sent over the network 
during the shuffle is greatly reduced.


Partitioning
------------
Comes into play when shuffling data around your cluster, partitioning your data 
intelligently can save time when running computations. It's important to understand
in general with distributed systems and in particular with park RDDs

The data within an RDD is split into several partitions:
	-Partitions are very rigid
	-Partitions never span multiple machines
	-Each machine in the cluster contains one or more partitions
	-The number of partitions to use is configurable. By default
	 is the total number of cores on all executor nodes.

Two kinds of partitions:
	-Hash partitioning
	-Range partitioning
	-Custom partitioning

* Only possible when working with Pair RDDs, since partitioning is done based on keys

1) Hash partitioning: Evenly distributed, computes partitions for every tuple in the
		      pair RDD. Start by getting the key's hashs code and modulo with
		      the default number of partitions

	- P = K.hashCode() % numPartitions

2) Range partitioning: Important when some kind of order is defined on the key.
	e.g. Int, Char, String.
	When working with pair RDDs that had keys that were integers, keys are
	partitioned according to two things.
	-Ordering for keys
	-Set of sorted ranges of keys

* Tuples with keys in the same range appear on the same machine

3) Custom partitioning: There are two ways to create RDDs with specific partitionings
	-PartitionBy creates an RDD, providing an explicit partitioner
	-Using transformations that return RDDs with specific partitioners

*Certain transformation use certain kinds of partitioners.


Spark functions
---------------
*reduceByKey: Combination of first groupByKey and then reduce-ing on all the values 
	grouped per key. Operates on the values that you assume are already grouped by 
	some key.

	-def reduceByKey(func: (V, V) => V): RDD[(K, V)]
	- .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2))


*groupByKey: Results in one key-value pair per key, and this single key-value pair
	cannot span across multiple worker nodes.


*partitioning data: Invoking partitionBy creates an RDD with a specified partitioner.
	-val pairs		= purchasedRdd.map(p => (p.cuntomerId, p.price))
	-val tunedPartitioner	= new RangePatitioner(8, pairs) ----> reference to RDD
	-val partitioned        = pairs.partitionBy(tunedPartitioner).persist()

*persist(): Due to Spark semantics and tendency to reevaluate chains of transformations
	    again and again, the data would be shuffled over the network and partitioned
	    again. Keeping it in memory

*Operations on pair RDDs that hold to and propagate a partitioner:
	-cogroup		-foldByKey
	-groupWith		-combineByKey
	-join			-partitionBy
	-leftOuterJoin		-sort
	-rightOuterJoin		-mapValues (if parent has a patitioner)
	-groupByKey		-flatMapValues (..)
	-reduceByKey		-filter (..)


* If you use map or flatMap on a partitioned RDD, that RDD will lose its partitioner 

* Organized data in a certain way across your cluster will lose its organization

* Its possible for map to change the keys in a pair RDD

* Hence MAPVALUES, enables to still do map transformations without changing the keys,
  preserving the partitioner.


Wide vs Narrow Dependencies
---------------------------
Dictates relationships between RDDs in graphs of computation,
which has alot to do with shuffling.

Not all Transformations are Equal:
Some transformations are more expensive in terms of latency than others.
E.g. Requiring lots of data to be transferred over the network


** wide transformations vs narrow transformations
** Shuffling-minimize
** Disk spilling
** partitions
** Catalize optimizer??

Communicate
-----------
Broadcast: Information in your driver/controller, pass to every executor
Take: Information the executors have, bring back to driver. Taking too 
much data can lead to problems, no scalability
DAG: Instruction, transmitted from driver to the executors. Control flow
logic is inexpensive operation.
Shuffle: Product of mapper and reducers, the intermediate files need to 
travel over the network and serialize

*Exchange needs to be done in a lock step manner 
*RDD, Dataframes
*DAGs
*flumeJava APIs
*Long lived jobs 


Spark Hierarchy - Hardware standpoint
-------------------------------------
Cluster: Driver, Executors

Executors: Cores (SLOTS), memory, disks


Cores: Insert tasks into

Memory: Storage and working
  -Working memory: Utilized by spark workloads
  -Storage memory: Persisted objects 

Disks: Attached directly and provide space for shuffle partitions and persistance
       to disk and spills from execution of the workloads.
       -Fast disks
       -Slow disks
       -Remove 
       -Local 

Spark Hierarchy - Software standpoint
-------------------------------------
Actions and transformations. Actions perform one or many transformations at a time 
which spanws jobs. Every tasks inside of a stage does the same thing only on another
segment of the data, if a task is required to do something different then it needs to
be inside of another stage. One task is operated by one core which is one partition
and one task takes up one slot.

Transformations: they are lazy, they dont do anything until an action is called.
  -Narrow
  -Wide (requires shuffle)

Spawn jobs: For orchestration & collections to coordinate tasks
  -Spawn stages: For orchestration & collection to coordinate tasks
    -Stages: contains tasks
      -Tasks: Do the work and utilize the hardware directly.


Minimize data scans (lazy loads)
--------------------------------
Data skipping:
  -Hive partitions
  -Bucketing
  -Databricks Delta Z-Ordering

Partitions
----------
Each of a number of portions into which some operating systems divide memory or storage.
Subset of a superset.

Spark partition: Data that is broken down so that it can be operated in parrallel in memory.
  -Input: You can control the size of the partition.
    -Spark default parallelism.
    -Spark max partition bytes.
  -Shuffle: You can only control the count.
    -Shuffle partitions: Default 200
  -Output: You can control the composition of the output
    -Coalesce: Shrink down the file set, never use repartition.
    -Repartition: To increate and/or rebalance (shuffle) --> carefull.
    -Df.write.option: Set number of records in each file.

*Partition Count = Stage Input Data / Target Size (< 200Mb)

Output partitions
-----------------
-Size of your files and how you want them structured
-Composition of files
-Maximise parallelism

Compaction
----------
Put files back together to the right size

Composition
-----------
Shuffle partitions, output partitions --> change compositions of the files

Balance
-------
Maximizing resources requires balance
  -Task duration
  -Partition size

Minimize data scans (persistence)
---------------------------------
Cash is a certain type of persist

Persistence: Not free, they take time and space, only persist when you have repetition
  -Cash: Goes to memory first then goes to disk.
  -Ram: Fast medium storage level
  -Dont forget to unpersist!
Repetition: SQL plan, identify super-sets that are used in many ways for sub-sets


Spills
------
Most of the time, the hardest work the cluster will deal with is the shuffle, most data intensive.
When you dont write size the shuffle partitions, you get spills, the slowest process of a job.

Persistence vs Broadcast
------------------------
Persistent: Take data from parititions and save them in executors, each executor dividing the load of the 
            the data in memory 
Broadcast: Take data from partitions and save them in working memory, pull the data through a collect
           into the driver and transformed it from a dataframe to a broadcastable object, similar to a
           hashmap and send all the transformed data back to each executor.Data in the driver will be 
           persistent until the garbage collector has run.

Omit expensive Ops
------------------
Repartition: Use coalesce or shuffle partition count
Count: Do you really need it?
DistinctCount: Use approxCountDistinct
-Use dropDuplicates
-Drop duplicates before the join
-Drop duplicates before the groupBy

Range Joins Types
-----------------
Right size beans and the right number of beans by time

-Point in interval range join: Predicate specifies value in one relation that is between two values from 
 the other relation
-Interval Overlap range join: Predicate specifies an overlap of intervals between two values from each 
 relation

Skew Optimization
-----------------
Salting method, will work for aggregates and for joins 
-Null safe equality: Its not going to try to join the nulls 

Join Optimization
-----------------
SortMergeJoins --> Both sides are large
Broadcast Join --> One side is small
  -Not enough driver memory
    -Validation functions
  -DF max result size
Skew joins
Range joins
BroadcastedNestedNestedLoop Join (BNLJ)






