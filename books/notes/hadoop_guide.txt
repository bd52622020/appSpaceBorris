
-----------------------------------------------------------------------------------------------------------
                                            Hadoop Guide
-----------------------------------------------------------------------------------------------------------
Preface
-------
Hadoop fundamentally an enabling technology for working with huge datasets. Hadoop provides a bridge between
structured (RDBMS) and unstructured (log files, xml, text) data and allows these datasets to be easily put
together.

Background and fundamentals
---------------------------
Hadoop's components and its ecosystem, how to store and work with voluminous data sizes and how to understand
data and turn it into a competitive advantage.

  -Hadoop provides effective storing and computational capabilities over substantial amounts of data.
  -Distributed system made up of a distributed file system and offers parallelization.
  -The computation tier uses a framework called MapReduce
  -A distributed filesystem called HDFS provides storage
  -Hadoop runs on commodity hardware

What is Hadoop
--------------
Hadoop is a platform that provides both distributed storage and computational capabilities. A distributed
master-slave architecture consisting of the Hadoop distributed file system (HDFS) for storage and MapReduce
for computational capabilities.

  -MapReduce, a computational framework for parallel processing
  -Data partitioning 
  -Its storage and computational capabilities scale with the addition of hosts to a Hadoop cluster 

High level Hadoop architecture
------------------------------
Master node: MapReduce for computation and HDFS for storage
  -MapReduce: Responsible for organizing where computational work should be scheduled on the slave nodes.
  -HDFS: Responsible for partitioning the storage across the slave nodes and keeping track of where data is
         located.

Slave node: 
  -MapReduce
  -Storage 

Core Hadoop components - HDFS
----------------------------
Storage component of Hadoop. Its a distributed filesystem that's modeled after Google file system (GFS).

  -HDFS is optimized for high throughput and works best when reading and writing large files (Gigabytes)
  -HDFS leaverages unusually large block sizes and data locality optimizations to reduce network input and
   output (I/O).
  -Scalability and availability are achieved due to data replication and fault tolerance.
  -HDFS replicates files for a configured number of times and re-replicates data blocks when nodes fail.

Core Hadoop components - MapReduce
----------------------------------
MapReduce is a batch-based, distributed computing framework, allows to parallelize work over a large amount
of raw data such as combining web logs with relational data from OLTP database to model how users interact 
with the website.

  -MapReduce simplifies parallel processing by abstracting away the complexities involved in working with 
   distributed systems such as computational paralelization, work distribution and unreliable hardware.
  -MapReduce allows to focus on addressing business needs.
  -MapReduce decomposes work submitted by a client into small parallelized map and reduce workers.


High level HDFS architecture
----------------------------
NameNode: Keeps in memory the metadata about the filesystem, such information about block management.
DataNode: Files are made up of blocks, each file is replicated and copies are stored in other nodes.
Client Application: HDFS client talk to the NameNode for metadata-related activities and read/writes.

MapReduce job 
-------------
The role of the programmer is to define map and reduce functions, where the map function outputs key/value
tuples, which are processed by reduce functions to produce the final output.

1) Client; Submits a MapReduce job to MapReduce Master.
2) MapReduce: Decomposes the job into map and reduce tasks and schedules them for remote execution on slave
              nodes.
    -Map: Takes as input a key/value pair, which represents a logical record from the input data source and
          in case of files it could be a line or a row in case of databases. Producing zero or more output
          key/value pairs or it could be performing a demultiplexing operation, where a single input key/
          value yields multiple key/value output pairs.

MapReduce shuffle and sort
--------------------------
The shuffle and sort phases are reponsible for two primary activities, determining the reducer that should 
receive the map output key/value pair (called partitioning) and ensuring that, for a given reducer, all its
input keys are sorted.

  -The power of MapReduce occurs in between the map output and the reduce input, in the shuffle and sort phases
  -The reduce function is called once per unique map output key.
  -All the map output values that were omitted across all the mappers for "key2" are provided in a list
  -Like the map function, the reduce can output zero to many key/value pairs. Reducer output can be written to
   flat files in HDFS, insert/update rows in a NoSQL database or write to any data sink.
  -E.g.reduce (key2, list(value2)) ---> list(key3, value3)

MapReduce logical architecture
------------------------------
1)Client application: Talk to the job tracker to launch and manage jobs.
2)Job tracker: Coordinates activities across the slave TaskTracker process. It accepts MapReduce job requests
               and schedules map and reduce tasks on TaskTrackers to perform the work.
3)TaskTracker: Is a daemon process that spawns child processes to perform the actual map or reduce work. They
               read their input from HDFS, and write their output to the local disk. Reduce tasks read the map
               outputs over the network and write their outputs back to HDFS.

The Hadoop ecosystem
--------------------
High level languages: Crunch, Cascading, Pig, Hive
Miscellaneous: Sqoop, Oozie, Flume, Hbase
Hadoop: HDFS, MapReduce

Physical architecture
---------------------
Zookeeper requires an odd-numbered quorum, at least three of them in any reasonably sized cluster. Extending 
the overview to include CPU, RAM, disk and network because they all have an impact on the throughput and 
performance of your cluster.

  -Client: Client hosts run application code in conjunction with the Hadoop ecosystem projects. Pig, Hive and
           Mahout are client-side projects that don't need to be installed on your actual Hadoop cluster.
  -Primary master: A single master node runs the master HDFS, MapReduce and HBase daemons. Running these master
                   on the same cluster is sufficient in small projects, with larger clusters it would be worth
                   considering splitting them onto separate hosts.
                   -NameNode
                   -Job tracker
                   -HMaster
                   -Zookeeper
  -Secondary master: Provides NameNode checkpoint management services, and zookeeper is used by HBase for 
                     metadata storage.
                     -Secondary NameNode
                     -Zookeeper
  -Zookeeper master: Instance of Zookeeper running
  -Slaves: The slave hosts run the slave daemons, to keep data locality, the ability to read from local disk, 
           which is a key distributed system property of both MapReduce and HBase slave daemons.
           -Data node
           -Task tracker
           -Region server
           -optional: RHIPE, R, RHadoop

Security
--------
Hadoop can be configured to run with Kerveros, a network authentication protocol, which requires Hadoop daemons
to authenticate clients, both user and other Hadoop components. Kerberos can be integrated with an organization
existing Active directory and thus offers a single sign-on experience for users.

Weakness
--------
HDFS: Lack of high availability, its inefficient handling of small files and lack of transparent compression.
      HDFS isn't designed to work well with random reads over small files due to its optimization for sustained
      throughput
MapReduce: Batch based architecture, no real-time data access.
High availability: Single master models, resulting in single point of failure.

Redhat
------
Uses packages called RPMs for installation, and Yum as a package installer that can fetch RPMs from remote Yum 
repositories. Clousera hosts its own Yum repository containing Hadoop RPMs

Hadoop configuration files
--------------------------
Hadoop-env.sh: Environment specific settings, current java home and you can also specify JVM options for various
               Hadoop components. Customizing directory locations such as the log directory and location of the 
               master and slve files.

Core-site.xml: Contains system-level Hadoop configuration items, such as HDFS url, Hadoop temporary directory and
               script locations for rack-aware Hadoop clusters.

Hdfs-site: Contains HDFS settings such as the default file replication count, the block size and whether
           permissions are enforced.

Mapred-site: HDFS settings such as the default number  of reduce tasks, default min/max task memory size and 
             speculative execution settings.

Masters: Contains a list of hosts that are Hadoop masters, Name should have been called temporary masters 

Slaves: Contains a list of hosts that are Hadoop slaves. When Hadoop starts, it will SSH to each host in this file
        and launch the DataNode and TaskTracker daemons.

Basic CLI commands
------------------
To list the files in the root directory in HDFS
-hadoop fs -ls / 
To make sure MapReduce is up and running
-hadoop job -list

Job
---
-When your job completes you can examine HDFS for the job output files, and also view their contents.
-The map and reduce logs files can be found with the job's ID, which will take you into the local lifesystem. 
 When you run your job, part of the output is the job ID
-One a true distributed cluster these logs will be local to the remote TaskTracker nodes, which can make it harder
 to get to them. The JobTracker and TaskTracker UI can help to provide easy access to the logs.

Summary
-------
Hadoop is a distributed system designed to process, generate, and store large datasets. Hadoop excels at working 
with heterogeneous structured and unstructured data at scale.

------------------------------------------------------------------------------------------------------------------
                                              Part 2
------------------------------------------------------------------------------------------------------------------
Data logistics 
--------------
Examples include working with relational data in RDBMSs, structured files and HBase

Moving data in and out of Hadoop
--------------------------------
Moving data in and out of Hadoop, known as data ingress and egress, is the process by which data is transported 
from an external system into and internal system, and vice versa.

  -Hadoop supports ingress and egress at a low level in HDFS and MapReduce
  -Files can be moved in and out of HDFS, and data can be pulled from external data sources and pushed to external
   data sinks using MapReduce.
  -How do you bring data that's sitting in an OLTP (online transaction processing) database, or ingress log data 
   that's being produced by tens of thousands of production servers, or work with binary data sitting behind a 
   firewall?

Example
-------
Data ingress, perform processing such as joining different data sources together and copy data out of Hadoop.

  -Log data --> Log collectors --> HDFS --> File egress --> Files
  -Files --> File ingress --> HDFS --> Files egress --> Files
  -HBase /NoSQL --> MapReduce --> HBase/NoSQL
  -OLTP --> MapReduce --> OLTP/OLAP

Automation of processes
-----------------------
Automation is a critical part of the process, along with monitoring and data integrity reponsibilities to ensure
correct and safe transportation of data. There are different tools that simplify the process of ferrying data in
and out of Hadoop and how to automate the movement of log files, ubiquitous data sources for Hadoop, but which 
tend to be scattered throughout your environments and therefore present a collection and aggregation challenge.

  -Flume for moving log data into Hadoop, and in the process we'll evaluate two competing log collection and
   aggregation tools, Chukwa and Scribe.
  -Sqoop for database ingress and egress activities, and we'll look at how to ingress and egress data in HBase

Key elements of ingress and egress
----------------------------------
Moving large quatities of data in and out of Hadoop has logitical challenges that include consistency guarantees 
and resource impacts on data sources and destinations. 
Design elements:

  -Idempotence: An indempotent operation produces the same result no matter how many times it's executed. In a 
                relational database the inserts typically aren't idempotent, because executing them multiple
                times doesn't produce the same resulting database state. Any time data is being written 
                idempotence should be a consideration, How well do distributed log collection frameworks deal
                with data retransmissions?. How to ensure idempotent behaviour in a MapReduce job where multiple
                tasks are inserting into a database in parallel?
  -Aggregation: The data aggregation process combines multiple data elements. In the context of data ingress this
                can be helpful because moving large quatities of small files into HDFS potentially translates 
                into NameNode memory woes, slow MapReduce execution times. Having the ability to aggregate files
                or data together mitigates the problem.
  -Data format: The data format transformation proces converts one data format into another. Often your source
                data isn't in a format that's ideal for processing in tools such as MapReduce. If your source 
                data is multiple XML or JSON form, for example, you may want to consider a preprocessing step.
                This would convert the data into a form that can be split, such as a JSON or an XML element per
                line, or convert it into a format such as Avro.
  -Recoverability: Allows an ingress or egress tool to retry in the event of failed operation. Because it's 
                unlikely that any data source, sink, or Hadoop itslef can be 100 percent available, its important
                that an ingress or egrss action be retried in the event of failure.
  -Correctness: In the context of data transportation, checking for correctness is how you verify that no data
                corruption occurred as the data was in transit. When you work with heterogeneous systems such as 
                Hadoop  data ingress and egress tools, the fact that data is being transported across different 
                hosts, networks, and protocols only increases the potential for problems during data transfer.
                Common methods for checking correctness of raw data such as storage devices include Cyclic 
                Redundancy Checks (CRC), which are what HDFS uses internally to maintain block-level integrity.
  -Resource consumption and performance: The measures of system resource utilization and system efficiency,
                respectively. Ingress and egress tools don't typically incur significant load/resource consumption
                on a system, unless you have appreciable data volumes. For performance, the questions to ask 
                include whether the tool performs ingress and egress activities in parallel, what mechanisms it
                provides to tune the amount of parallelism. For example if your data source is a production databse,
                don't use a large number of concurrent map task to import data.
  -Monitoring: Ensures that functions are performing as expected in automated systems. For data ingress and egress,
                monitoring breaks down into two elements: Ensuring that the processes involved in ingress and egress
                are live, and validating that source and destination data are being produced as expected.

Moving data into Hadoop
-----------------------
The firs step is to make the data available to Hadoop. There are two primary methods that can be used for moving 
data into Hadoop. Reading data in MapReduce has advantages in the ease with which the operation can be parallelized
and fault tolerant. Not all data is accessible from MapReduce, however, such as in the case of log files, which is 
where other systems need to be relied upon for transportation, including HDFS for the final data hop.

  -Writing external data at the HDFS level (a data push)
  -Reading external data at the MapReducelevel (like a pull)

Low-level Hadoop ingress mechanisms
-----------------------------------
Focus on high-level data ingress tools that provide easy and automated mechanisms to get data into Hadoop. All these
tools use one of a finite set of low-level mechanisms, however, which Hadoop provides to get data in and out.
These mechanisms include Hadoop's Java HDFS API, WebHDFS, the new Hadoop REST API, and MapReduce.

Pushing log files into Hadoop
-----------------------------
Log data has long been prevalent across all applications, but with Hadoop came the ability to process the large 
volumes of log data procuded by production systems. Various systems produce log data, from network devices and 
operating systems to web servers and applications. These log files all offer the potential for valuable insights 
into how systems and applications operate as well as how they're used.

  -What unifies log files is taht they tend to be in text form and line-oriented, making them easy to process.
  -Tools to transport log data from source to HDFS
  -How to transport system log files into HDFS and Hive
  -Deploy, configure, and run an automated log collection and distribution infrastructure

Log collection tools: Flume, Chukwa and scribe are log collection and distribution frameworks that have the 
                  capability to use HDFS as a data sink for that log data. 
  -Flume: Apache Fule is a distributed system for collecting streaming data. It's an Apache project in incubator 
          status, originally developed by Cloudera. It offers various levels of reliability and transport delivery
          guarantees that can be tuned to your needs. It's customizable and supports a plugin architecture to add
          custom data sources and data sinks.
































